{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"nL4K4sLBq7D9","collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719580623920,"user_tz":-330,"elapsed":30026,"user":{"displayName":"Chetanya Bhandawat","userId":"09021226186098531371"}},"outputId":"c2c93fa1-8916-4b69-bfaf-8fbc55027e71"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n"]}],"source":["# Installing libraries\n","!pip install beautifulsoup4\n","!pip install requests"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":15266,"status":"ok","timestamp":1719580639175,"user":{"displayName":"Chetanya Bhandawat","userId":"09021226186098531371"},"user_tz":-330},"id":"e-pFNE3Krf_1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"678fe868-58e3-4b4c-d728-d0a579845515"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"source":["# Importing relevant libraries\n","from bs4 import BeautifulSoup, NavigableString\n","import requests\n","import re\n","import pandas as pd\n","import sqlite3\n","import string\n","from nltk.corpus import wordnet as wn\n","import itertools\n","import spacy\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"markdown","metadata":{"id":"jDUPHBWPXKTF"},"source":["# **WEB SCRAPING**"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"0NrrFX_IJe7Q","executionInfo":{"status":"ok","timestamp":1719580639176,"user_tz":-330,"elapsed":57,"user":{"displayName":"Chetanya Bhandawat","userId":"09021226186098531371"}}},"outputs":[],"source":["def scrape_page(url):\n","    # Send a GET request to the URL\n","    response = requests.get(url)\n","\n","    # Check if the request was successful\n","    if response.status_code == 200:\n","        # Parse the HTML content of the page\n","        doc = BeautifulSoup(response.content, 'html.parser')\n","\n","        # Return the scraped data\n","        return doc\n","\n","    # If the request was not successful, return None\n","    return None\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"X0YeLzBNtoZd","executionInfo":{"status":"ok","timestamp":1719580640907,"user_tz":-330,"elapsed":1782,"user":{"displayName":"Chetanya Bhandawat","userId":"09021226186098531371"}}},"outputs":[],"source":["# Getting the HTML from the URL using BeautifulSoup\n","doc_main = scrape_page(\"https://www.gov.wales/land-transaction-tax\")\n","#print(doc_main.prettify())"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"-SK9J7WedjNL","executionInfo":{"status":"ok","timestamp":1719580640908,"user_tz":-330,"elapsed":34,"user":{"displayName":"Chetanya Bhandawat","userId":"09021226186098531371"}}},"outputs":[],"source":["h2_heading = doc_main.find('h2', class_='list-group__title')\n","if h2_heading:\n","  # Find all the links under the <h2> heading\n","  links = h2_heading.find_next('ul').find_all('a')\n","\n","  # Extract the URLs from the links\n","  link_urls = [link['href'] for link in links]\n","\n","#print(len(link_urls))"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"ZmZyzEMtaGmW","executionInfo":{"status":"ok","timestamp":1719580671995,"user_tz":-330,"elapsed":31116,"user":{"displayName":"Chetanya Bhandawat","userId":"09021226186098531371"}}},"outputs":[],"source":["#Declaring empty lists\n","urls = []\n","heading = []\n","que_lst = []\n","para_lst = []\n","\n","for url in link_urls:\n","  # Calling the scrape_page function to scrape the url\n","  doc = scrape_page(url)\n","  # Getting the page headings\n","  page_heading = doc.find(\"h1\", class_ = 'page-header__title page-header__title--has-type').find(string=True)\n","\n","  # Getting the headings\n","  section_class = \"paragraph paragraph--type--content-section paragraph--view-mode--default\"\n","  doc_heading = doc.find_all(\"div\", section_class)\n","\n","  # Getting the documents if exist\n","  doc_div = doc.find_all(\"div\",class_ = \"document--accessible document\")\n","  doc_title = doc.find_all(\"h3\", class_ = \"document__title\")\n","\n","  # Getting the button links\n","  btn_div = doc.find_all(\"div\", class_ =\"btn--launcher\")\n","\n","  # If the headings exist it will append it to question list\n","  if(doc_heading):\n","    que_lst.extend([h2.get_text(strip=True) for div in doc_heading for h2 in div.find_all(\"h2\")])\n","\n","    # Getting the paragraphs of the respective headings\n","    para_elements = []\n","    for section in doc_heading:\n","      content = section.find_next('div', class_ = True)\n","      merged_content = ''\n","      content_class = content['class']\n","\n","      while(len(content_class)!=0 and 'paragraph--type--content-section' not in content_class and 'col-md-4' not in content_class):\n","        merged_content += content.get_text(separator=\" \") + \" \"\n","        content = content.find_next('div', class_ = True) if content else ''\n","        content_class = content['class'] if content else ''\n","\n","      para_elements.append(merged_content)\n","\n","  #If there are no headings we will only take the paragraphs and assign empty string to the question list\n","  else:\n","    content_class = \"paragraph paragraph--type--content paragraph--view-mode--default\"\n","    div_content_elements = doc.find_all(\"div\", content_class)\n","    para_elements = []\n","\n","    for content in div_content_elements:\n","      merged_content = content.get_text(separator=\" \") + \" \"\n","      para_elements.append(merged_content)\n","      que_lst.append(\" \")\n","\n","  # Appending the paragraphs to the Answer list\n","  for div in para_elements:\n","        link_content = \"\"\n","        for link in doc.find_all(\"a\"):\n","            link_text = link.get_text()\n","            link_url = link.get(\"href\")\n","            link_content += f\"[{link_text}]({link_url})\"\n","            div = div.replace(link_text, f\"{link_text}({link_url})\")\n","        para_lst.append(div.split('\\n', 1)[1] if '\\n' in div else div)\n","        heading.append(page_heading)\n","        urls.append(url)\n","\n","  # If there is a document present inside the link it will add its link\n","  if(doc_div):\n","    for index, div in enumerate(doc_div):\n","      span = doc_title[index].find('span')\n","      para_lst.append(span.get_text(strip=True) + \" (\" + div.find('a')['href'] + \")\")\n","      que_lst.append(\"DOCUMENT\")\n","      heading.append(page_heading)\n","      urls.append(url)\n","\n","  # If there are button links present it will ad its link\n","  if(btn_div):\n","    for div in btn_div:\n","      para_lst.append(div.get_text(strip=True) + \" (\"+ div.find('a')['href'] + \")\")\n","      que_lst.append(\"BUTTON LINK\")\n","      heading.append(page_heading)\n","      urls.append(url)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"wxqXdKQb3F5e","executionInfo":{"status":"ok","timestamp":1719580671996,"user_tz":-330,"elapsed":29,"user":{"displayName":"Chetanya Bhandawat","userId":"09021226186098531371"}}},"outputs":[],"source":["# There are pages like \"Land Transaction Tax for professionals\" and \"Tax collection and management: technical guidance\" which have sub-links\n","# Taking the URLs which have sub-links inside them\n","nested_url = ['https://www.gov.wales/land-transaction-tax-professionals', 'https://www.gov.wales/tax-collection-and-management-technical-guidance']\n","sub_heading_url = []\n","sub_heading_name = []\n","main_headings = []\n","\n","# Scraping the URLs to get sub-links URLs\n","for url in nested_url:\n","  nested_url_doc = doc = scrape_page(url)\n","  h2_div_class = nested_url_doc.find_all(\"div\",class_ = \"paragraph paragraph--type--collection-section paragraph--view-mode--default collection\")\n","  page_heading = doc.find(\"h1\", class_ = 'page-header__title page-header__title--has-type').find(string=True)\n","\n","  for h2_div in h2_div_class:\n","    h2_tags = h2_div.find(\"h2\")\n","    content = h2_div.find_next(\"div\", class_=True)\n","    content_class = content['class']\n","\n","    while(len(content_class)!=0 and 'paragraph--type--collection-section' not in content_class and 'col-md-4' not in content_class):\n","\n","      if('index-list__title' in content_class):\n","        sub_heading_url.append(content.find('a')['href'])\n","        main_headings.append(page_heading + \" / \" + h2_tags.get_text(strip = True))\n","        sub_heading_name.append(content.find('span').get_text(strip=True))\n","\n","      content = content.find_next('div', class_ = True) if content else ''\n","      content_class = content['class'] if content else ''\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Ovr_McCq1wqg","executionInfo":{"status":"ok","timestamp":1719580717457,"user_tz":-330,"elapsed":45488,"user":{"displayName":"Chetanya Bhandawat","userId":"09021226186098531371"}}},"outputs":[],"source":["# Initialize i to 0\n","i=0\n","\n","for url in sub_heading_url:\n","  # Calling the scrape_page function to scrape the page\n","  doc = scrape_page(url)\n","  #print(url)\n","\n","  # Getting the headings\n","  section_class = \"paragraph paragraph--type--content-section paragraph--view-mode--default\"\n","  doc_heading = doc.find_all(\"div\", section_class)\n","\n","  # Getting documents\n","  doc_div = doc.find_all(\"div\",class_ = \"document--accessible document\")\n","  doc_title = doc.find_all(\"h3\", class_ = \"document__title\")\n","\n","  if(doc_heading):\n","    que_lst.extend([h2.get_text(strip=True) for div in doc_heading for h2 in div.find_all(\"h2\")])\n","\n","    # Getting the paragraphs of the respective headings\n","    para_elements = []\n","    for section in doc_heading:\n","      content = section.find_next('div', class_ = True)\n","      merged_content = ''\n","      content_class = content['class']\n","\n","      while(len(content_class)!=0 and 'paragraph--type--content-section' not in content_class and 'col-md-4' not in content_class):\n","        merged_content += content.get_text(separator=\" \") + \" \"\n","        content = content.find_next('div', class_ = True) if content else ''\n","        content_class = content['class'] if content else ''\n","\n","      para_elements.append(merged_content)\n","\n","  # If there are no headings we will only take the paragraphs and assign empty string to the question list\n","  else:\n","    content_class = \"paragraph paragraph--type--content paragraph--view-mode--default\"\n","    div_content_elements = doc.find_all(\"div\", content_class)\n","\n","    para_elements = []\n","    for content in div_content_elements:\n","      merged_content = content.get_text(separator=\" \") + \" \"\n","      para_elements.append(merged_content)\n","      que_lst.append(\" \")\n","\n","  # Appending the paragraphs to the Answer list\n","  for div in para_elements:\n","    link_content = \"\"\n","    for link in doc.find_all(\"a\"):\n","        link_text = link.get_text()\n","        link_url = link.get(\"href\")\n","        link_content += f\"[{link_text}]({link_url})\"\n","        div = div.replace(link_text, f\"{link_text}({link_url})\")\n","\n","    para_lst.append(div.split('\\n', 1)[1] if '\\n' in div else div)\n","    heading.append(main_headings[i] + \" / \" + sub_heading_name[i])\n","    urls.append(url)\n","\n","\n","  # If there is a document present inside the link it will add its link\n","  if(doc_div):\n","    for index, div in enumerate(doc_div):\n","      span = doc_title[index].find('span')\n","      para_lst.append(span.get_text(strip=True) + \" (\" + div.find('a')['href'] + \") \")\n","      que_lst.append(\"DOCUMENT\")\n","      heading.append(main_headings[i] + \" / \" + sub_heading_name[i])\n","      urls.append(url)\n","  i=i+1\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"9B5-5IzxQQ0V","executionInfo":{"status":"ok","timestamp":1719580717459,"user_tz":-330,"elapsed":79,"user":{"displayName":"Chetanya Bhandawat","userId":"09021226186098531371"}}},"outputs":[],"source":["# Set display options to show the complete text\n","pd.set_option('display.max_colwidth', None)\n","\n","# Convert the headings and paragraphs in a question-answer format and storing them in a dataframe\n","df = pd.DataFrame({'Heading': heading,'Questions': que_lst, 'Answers': para_lst, 'URLs':urls})\n","df.reset_index(inplace=True)\n","df.rename(columns={'index': 'Index'}, inplace=True)\n","\n","# Storing it in CSV File\n","df.to_csv(\"General_Guidance.csv\")\n"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOxbparSPEOSUR/7jFfBz8q"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}